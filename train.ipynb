{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# set the configuration\n",
    "import configparser\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "\n",
    "from models import UNet\n",
    "from models.deeplab import get_deeplab\n",
    "\n",
    "\n",
    "import seg_data\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "#         self.img_labels = pd.read_csv(annotations_file)\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "#         image = read_image(img_path)\n",
    "#         label = self.img_labels.iloc[idx, 1]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "#         return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dice coefficient\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size()\n",
    "    assert input.dim() == 3 or not reduce_batch_first\n",
    "\n",
    "    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n",
    "\n",
    "    inter = 2 * (input * target).sum(dim=sum_dim)\n",
    "    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n",
    "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
    "\n",
    "    dice = (inter + epsilon) / (sets_sum + epsilon)\n",
    "    return dice.mean()\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n",
    "\n",
    "\n",
    "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,model_name, dataloader, device, amp = True):\n",
    "    \"\"\"Evaluate the validation set\n",
    "\n",
    "    Return validation score (dice score).\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_val_batches = len(dataloader)\n",
    "    dice_score = 0\n",
    "    valid_loss = 0\n",
    "    # iterate over the validation set\n",
    "    with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "        for batch in dataloader:\n",
    "            images, mask_true = batch[0], batch[1]\n",
    "\n",
    "            # move images and labels to correct device and type\n",
    "            images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "            mask_true = mask_true.to(device=device, dtype=torch.float32)\n",
    "\n",
    "            # predict the mask\n",
    "            mask_pred = model(images)\n",
    "\n",
    "            if model_name==\"deeplab\":\n",
    "                mask_pred = mask_pred['out']\n",
    "\n",
    "        if model.n_classes == 1:\n",
    "            \n",
    "            loss += dice_loss(F.sigmoid(masks_pred.squeeze(1)), true_masks.float(), multiclass=False)\n",
    "        else:\n",
    "            loss = criterion(masks_pred, true_masks)\n",
    "\n",
    "            if model.n_classes == 1:\n",
    "                assert mask_true.min() >= 0 and mask_true.max() <= 1, 'True mask indices should be in [0, 1]'\n",
    "                valid_loss += criterion(masks_pred.squeeze(1), mask_true.float())\n",
    "\n",
    "                mask_pred = (F.sigmoid(mask_pred) > 0.5).float()\n",
    "                # compute the Dice score\n",
    "                dice_score += dice_coeff(mask_pred, mask_true, reduce_batch_first=False)\n",
    "            else:\n",
    "                assert mask_true.min() >= 0 and mask_true.max() < model.n_classes, 'True mask indices should be in [0, n_classes['\n",
    "                # convert to one-hot format\n",
    "                valid_loss += criterion(masks_pred, true_masks)\n",
    "                mask_true = F.one_hot(mask_true.argmax(dim=1).to(torch.long), model.n_classes).permute(0, 3, 1, 2).float()\n",
    "                mask_pred = F.one_hot(mask_pred.argmax(dim=1), model.n_classes).permute(0, 3, 1, 2).float()\n",
    "                # compute the Dice score, ignoring background\n",
    "                dice_score += multiclass_dice_coeff(mask_pred[:, 1:], mask_true[:, 1:], reduce_batch_first=False)\n",
    "\n",
    "    model.train()\n",
    "    return dice_score / max(num_val_batches, 1) , valid_loss/ max(num_val_batches, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR\n",
      "('image_dir', './data/training_suture_mini/img')\n",
      "('mask_dir', './data/training_suture_mini/mask')\n",
      "('checkpoint_path', './checkpoint/')\n",
      "('log_dir', './logs/')\n",
      "PARAMS\n",
      "('model', 'unet')\n",
      "('start_class_i', '1')\n",
      "('scale', '5')\n",
      "('n_classes', '3')\n",
      "('epochs', '30')\n",
      "('learning_rate', '0.01')\n",
      "('batch_size', '4')\n",
      "('val_percent', '20')\n",
      "UNET\n",
      "('bilinear', 'TRUE')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "## Read the config\n",
    "def read_ini(file_path):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(file_path)\n",
    "    return config\n",
    "\n",
    " \n",
    "config = read_ini(\"./train_config.ini\")\n",
    "\n",
    "for section in config.sections():\n",
    "    print(section)\n",
    "    for key in config[section]:\n",
    "        print((key, config[section][key]))\n",
    "\n",
    "\n",
    "img_path = config[\"DIR\"][\"image_dir\"]\n",
    "mask_path =config[\"DIR\"][\"mask_dir\"]\n",
    "checkpoint_path = config[\"DIR\"][\"checkpoint_path\"]\n",
    "\n",
    "start_class_i = int(config[\"PARAMS\"].get('start_class_i',0))\n",
    "model_name = config[\"PARAMS\"]['model']\n",
    "scale = int(config[\"PARAMS\"][\"scale\"])\n",
    "\n",
    "learning_rate = float(config[\"PARAMS\"]['learning_rate'])\n",
    "batch_size = int(config[\"PARAMS\"]['batch_size'])\n",
    "val_percent = int(config[\"PARAMS\"]['val_percent'])\n",
    "\n",
    "# number of classes\n",
    "n_classes = int(config[\"PARAMS\"].get('n_classes',False))\n",
    "# Class weight, if not specified, assign None\n",
    "class_weights = config[\"PARAMS\"].get('class_weights',False)\n",
    "if class_weights:\n",
    "    \n",
    "    class_weights = eval(class_weights)\n",
    "    assert len(class_weights) == n_classes, \"The length of class weights: {} should equal to the number of classes: {}\".format(len(class_weights),n_classes)\n",
    "    class_weights_tensor = torch.tensor(class_weights)\n",
    "    class_weights = True\n",
    "\n",
    "epochs = int(config[\"PARAMS\"]['epochs'])\n",
    "\n",
    "\n",
    "\n",
    "# n_classes = config\n",
    "\n",
    "# config.get(\"UNET\")\n",
    "if model_name == \"unet\" and \"UNET\" in config:\n",
    "    bilinear = config[\"UNET\"][\"bilinear\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "        Image folder: ./data/training_suture_mini/img\n",
      "        input image scale: 5\n",
      "        Mask folder: ./data/training_suture_mini/mask\n",
      "        Dataset length: 13\n",
      "        Validation set percentage: 20\n",
      "        Batch size: 4\n",
      "    \n",
      "System info:\n",
      "        Using device: cuda\n",
      "        CPU cores: 16\n",
      "        GPU count: 1\n",
      "\n",
      "No class weight in loss\n",
      "Model info:\n",
      "        Model name: unet\n",
      "        \n",
      "        Output channels: 3\n",
      "        Epochs: 30\n",
      "        Learning Rate: 0.01\n",
      "        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and init datasets\n",
    "dataset_whole = seg_data.segDataset(img_path = img_path, \n",
    "    mask_path = mask_path, n_classes=n_classes,\n",
    "    scale=scale , start_class_i = start_class_i)\n",
    "\n",
    "# dataset = seg_data.segDataset(img_path = img_path, \n",
    "#     mask_path = mask_path,\n",
    "#     scale=scale)\n",
    "\n",
    "# Split and create dataloader\n",
    "l=dataset_whole.__len__()\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset_whole)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset_whole, indices[:-int(np.ceil(l*val_percent/100))])\n",
    "dataset_val = torch.utils.data.Subset(dataset_whole, indices[int(-np.ceil(l*val_percent/100)):])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset_val, batch_size=1,\n",
    "                                        shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f'''Dataset info:\n",
    "        Image folder: {img_path}\n",
    "        input image scale: {scale}\n",
    "        Mask folder: {mask_path}\n",
    "        Dataset length: {l}\n",
    "        Validation set percentage: {val_percent}\n",
    "        Batch size: {batch_size}\n",
    "    ''')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'''System info:\n",
    "        Using device: {device}\n",
    "        CPU cores: {os.cpu_count()}\n",
    "        GPU count: {torch.cuda.device_count()}\n",
    "'''\n",
    ")\n",
    "\n",
    "\n",
    "# init model and device\n",
    "\n",
    "\n",
    "\n",
    "### Unet ###\n",
    "if model_name =='unet':\n",
    "    model = UNet(n_channels=3, n_classes=n_classes, bilinear=bilinear)\n",
    "elif model_name ==\"deeplab\":\n",
    "    model = get_deeplab(n_classes)\n",
    "    \n",
    "# switch NCHW to NHWC\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "model.to(device=device)\n",
    "\n",
    "# Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "optimizer = optim.RMSprop(model.parameters(),\n",
    "                            lr=learning_rate, foreach=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)  # goal: maximize Dice score\n",
    "# grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "# Loss function\n",
    "if class_weights:\n",
    "    print(\"Using class weight in loss: {}\".format(class_weights_tensor))\n",
    "    class_weights_tensor = class_weights_tensor.to(device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight= class_weights_tensor) if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "# Loss function with class weight\n",
    "else:\n",
    "    print(\"No class weight in loss\")\n",
    "    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "scaler=None\n",
    "\n",
    "# Learning rate warm up\n",
    "warmup_factor = 1.0 / 1000\n",
    "warmup_iters = min(1000, len(train_loader) - 1)\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'''Model info:\n",
    "        Model name: {model_name}\n",
    "        \n",
    "        Output channels: {n_classes}\n",
    "        Epochs: {epochs}\n",
    "        Learning Rate: {learning_rate}\n",
    "        \n",
    "''')\n",
    " \n",
    "#{\"Bilinear\" if model.bilinear else \"Transposed conv\"} upscaling\n",
    "\n",
    "\n",
    "\n",
    "# if args.load:\n",
    "#     state_dict = torch.load(args.load, map_location=device)\n",
    "#     del state_dict['mask_values']\n",
    "#     model.load_state_dict(state_dict)\n",
    "#     logging.info(f'Model loaded from {args.load}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No class weight in loss\n",
      "Model info:\n",
      "        Model name: unet\n",
      "        \n",
      "        Output channels: 3\n",
      "        Epochs: 30\n",
      "        Learning Rate: 0.01\n",
      "        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init model and device\n",
    "\n",
    "\n",
    "\n",
    "### Unet ###\n",
    "if model_name =='unet':\n",
    "    model = UNet(n_channels=3, n_classes=n_classes, bilinear=bilinear)\n",
    "elif model_name ==\"deeplab\":\n",
    "    model = get_deeplab(n_classes)\n",
    "    \n",
    "# switch NCHW to NHWC\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "model.to(device=device)\n",
    "\n",
    "# Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "optimizer = optim.RMSprop(model.parameters(),\n",
    "                            lr=learning_rate, foreach=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)  # goal: maximize Dice score\n",
    "# grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "# Loss function\n",
    "if class_weights:\n",
    "    print(\"Using class weight in loss: {}\".format(class_weights_tensor))\n",
    "    class_weights_tensor = class_weights_tensor.to(device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight= class_weights_tensor) if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "# Loss function with class weight\n",
    "else:\n",
    "    print(\"No class weight in loss\")\n",
    "    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "scaler=None\n",
    "\n",
    "# Learning rate warm up\n",
    "warmup_factor = 1.0 / 1000\n",
    "warmup_iters = min(1000, len(train_loader) - 1)\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'''Model info:\n",
    "        Model name: {model_name}\n",
    "        \n",
    "        Output channels: {n_classes}\n",
    "        Epochs: {epochs}\n",
    "        Learning Rate: {learning_rate}\n",
    "        \n",
    "''')\n",
    " \n",
    "#{\"Bilinear\" if model.bilinear else \"Transposed conv\"} upscaling\n",
    "\n",
    "\n",
    "\n",
    "# if args.load:\n",
    "#     state_dict = torch.load(args.load, map_location=device)\n",
    "#     del state_dict['mask_values']\n",
    "#     model.load_state_dict(state_dict)\n",
    "#     logging.info(f'Model loaded from {args.load}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/30\n",
      "torch.Size([4, 3, 316, 310]) torch.Size([4, 3, 316, 310])\n",
      "torch.Size([4, 3, 316, 310]) torch.Size([4, 3, 316, 310])\n",
      "torch.Size([2, 3, 316, 310]) torch.Size([2, 3, 316, 310])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 55\u001b[0m\n\u001b[0;32m     51\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     54\u001b[0m \u001b[39m# Validation stage\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m val_score \u001b[39m=\u001b[39m evaluate(model,model_name, test_loader, device)\n\u001b[0;32m     56\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mLoss/Valid\u001b[39m\u001b[39m'\u001b[39m, loss, epoch)\n\u001b[0;32m     57\u001b[0m scheduler\u001b[39m.\u001b[39mstep(val_score)    \n",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, model_name, dataloader, device, amp)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# iterate over the validation set\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautocast(device\u001b[39m.\u001b[39mtype \u001b[39mif\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m, enabled\u001b[39m=\u001b[39mamp):\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     12\u001b[0m         images, mask_true \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m], batch[\u001b[39m1\u001b[39m]\n\u001b[0;32m     14\u001b[0m         \u001b[39m# move images and labels to correct device and type\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "# 5. Begin training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print(\"Training epoch: {}/{}\".format(epoch,epochs))\n",
    "    for batch in train_loader:\n",
    "        images, true_masks = batch[0], batch[1]\n",
    "\n",
    "        # assert images.shape[1] == model.n_channels, \\\n",
    "        #     f'Network has been defined with {model.n_channels} input channels, ' \\\n",
    "        #     f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
    "        #     'the images are loaded correctly.'\n",
    "        \n",
    "\n",
    "        images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "        true_masks = true_masks.to(device=device, dtype=torch.float32)\n",
    "        \n",
    "\n",
    "        masks_pred = model(images)\n",
    "\n",
    "        if model_name ==\"deeplab\":\n",
    "            masks_pred = masks_pred['out']\n",
    "\n",
    "        print(true_masks.shape, masks_pred.shape)\n",
    "        if model.n_classes == 1:\n",
    "            loss = criterion(masks_pred.squeeze(1), true_masks.float())\n",
    "            loss += dice_loss(F.sigmoid(masks_pred.squeeze(1)), true_masks.float(), multiclass=False)\n",
    "        else:\n",
    "            loss = criterion(masks_pred, true_masks)\n",
    "            loss += dice_loss(\n",
    "                F.softmax(masks_pred, dim=1).float(),\n",
    "                F.one_hot(true_masks.argmax(dim=1), model.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                multiclass=True\n",
    "            )\n",
    "        writer.add_scalar('Loss/train', loss, epoch)\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch == 0:\n",
    "            warm_up_scheduler.step()\n",
    "        \n",
    "        # Update the global step and epoch loss\n",
    "        global_step += 1\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    \n",
    "    # Validation stage\n",
    "    # Validation stage\n",
    "    val_score,val_loss = evaluate(model,model_name, test_loader, device)\n",
    "    # writer.add_scalar('Loss/Valid', epoch_loss, epoch)\n",
    "    writer.add_scalar('Loss/Valid' , val_loss, epoch)\n",
    "    writer.add_scalar('Dice score/Valid' , val_score, epoch)\n",
    "    scheduler.step(val_score)    \n",
    "\n",
    " \n",
    "    Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "    state_dict = model.state_dict()\n",
    "    # state_dict['mask_values'] = dataset.mask_values\n",
    "    torch.save(model, str(checkpoint_path + '/{}_checkpoint_epoch{}.pth'.format(model_name, epoch)))\n",
    "    print(f'Checkpoint {epoch} saved! in {checkpoint_path}')\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 316, 310])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m test_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(dataset_val, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m      2\u001b[0m                                         shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m      7\u001b[0m         images, true_masks \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m], batch[\u001b[39m1\u001b[39m]\n\u001b[0;32m      8\u001b[0m         \u001b[39mprint\u001b[39m(true_masks\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\csyic\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset_val, batch_size=1,\n",
    "                                        shuffle=False, pin_memory=True)\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "  \n",
    "    for batch in test_loader:\n",
    "        images, true_masks = batch[0], batch[1]\n",
    "        print(true_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x1db197ff640>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 10, 4]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bettongia_penicillata_all_sutures_0095.tiff', 'Bettongia_penicillata_all_sutures_0115.tiff', 'Bettongia_penicillata_all_sutures_0135.tiff', 'Bettongia_penicillata_all_sutures_0155.tiff', 'Bettongia_penicillata_all_sutures_0175.tiff', 'Bettongia_penicillata_all_sutures_0195.tiff', 'Bettongia_penicillata_all_sutures_0215.tiff', 'Bettongia_penicillata_all_sutures_0235.tiff', 'Bettongia_penicillata_all_sutures_0255.tiff', 'Bettongia_penicillata_all_sutures_0275.tiff', 'Bettongia_penicillata_all_sutures_0295.tiff', 'Bettongia_penicillata_all_sutures_0315.tiff', 'Bettongia_penicillata_all_sutures_0335.tiff']\n",
      "['Bettongia_penicillata_all_sutures_0095.tiff', 'Bettongia_penicillata_all_sutures_0115.tiff', 'Bettongia_penicillata_all_sutures_0135.tiff', 'Bettongia_penicillata_all_sutures_0155.tiff', 'Bettongia_penicillata_all_sutures_0175.tiff', 'Bettongia_penicillata_all_sutures_0195.tiff', 'Bettongia_penicillata_all_sutures_0215.tiff', 'Bettongia_penicillata_all_sutures_0235.tiff', 'Bettongia_penicillata_all_sutures_0255.tiff', 'Bettongia_penicillata_all_sutures_0275.tiff', 'Bettongia_penicillata_all_sutures_0295.tiff', 'Bettongia_penicillata_all_sutures_0315.tiff', 'Bettongia_penicillata_all_sutures_0335.tiff']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "dataset = seg_data.segDataset(img_path = img_path, \n",
    "    mask_path = mask_path, n_classes=n_classes,\n",
    "    scale=scale , start_class_i = start_class_i)\n",
    "\n",
    "print(dataset.masks)\n",
    "print(dataset.imgs)\n",
    "print(dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.0863, 0.0863, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0863, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0863, 0.0824,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           ...,\n",
       "           [0.0902, 0.0902, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0902, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0902, 0.0902,  ..., 0.0902, 0.0902, 0.0902]],\n",
       " \n",
       "          [[0.0863, 0.0863, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0863, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0863, 0.0824,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           ...,\n",
       "           [0.0902, 0.0902, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0902, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0902, 0.0902,  ..., 0.0902, 0.0902, 0.0902]],\n",
       " \n",
       "          [[0.0863, 0.0863, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0863, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0863, 0.0824,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           ...,\n",
       "           [0.0902, 0.0902, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0902, 0.0902,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.0902, 0.0902, 0.0902,  ..., 0.0902, 0.0902, 0.0902]]]]),\n",
       " tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]], dtype=torch.float64)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import torchvision.transforms as T\n",
    "image = cv2.imread(\"data/suture/img/M1907_0215.tiff\", cv2.IMREAD_COLOR )\n",
    "\n",
    "# img_temp = Image.open(\"data/suture/img/M1907_0215.tiff\")\n",
    "# np.unique(img_temp.astype('uint8'))\n",
    "image\n",
    "\n",
    "# T.ToTensor(image)\n",
    "\n",
    "torch.from_numpy(image)\n",
    "# image.shape[:2] \n",
    "# image.resize(( 1, 2))\n",
    "\n",
    "mask = Image.open(\"data/suture/mask/M1907_0215.tiff\").convert('L')\n",
    "# mask = mask.resize((w_resized , h_resized))\n",
    "w_mask,h_mask = mask.size\n",
    "mask = np.array(mask)\n",
    "np.unique(mask)\n",
    "\n",
    "mask_temp = np.zeros((h_mask,w_mask,2))\n",
    "for i_class in range(2):\n",
    "    mask_temp[...,i_class] = np.where(mask == i_class, mask_temp[...,i_class], 1)\n",
    "#     # mask_temp[...,i_class] = np.where(mask != i_class, mask_temp[...,i_class], 1)\n",
    "mask_temp[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(mask == 0,1 ,mask_temp[...,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bde6227ca098a28506cae2de5e5d199190f968e09af08cbdae81bc10cc850e31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
